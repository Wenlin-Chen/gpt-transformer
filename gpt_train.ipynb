{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a941250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_models import GPT\n",
    "from gpt_utils import process_text, get_batch, estimate_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 64\n",
    "embed_size = 96\n",
    "n_heads = 4\n",
    "n_layers = 3\n",
    "dropout = 0.2\n",
    "bias = False\n",
    "\n",
    "max_iters = 5000\n",
    "lr = 3e-4\n",
    "eval_iters = 100\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab_size, encode, decode = process_text(text)\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 0.338688M\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(vocab_size, n_layers, n_heads, embed_size, block_size, dropout, bias)\n",
    "gpt = gpt.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 4.1944, val loss 4.1910\n",
      "step 100: train loss 2.8179, val loss 2.8313\n",
      "step 200: train loss 2.6406, val loss 2.6302\n",
      "step 300: train loss 2.5465, val loss 2.5529\n",
      "step 400: train loss 2.5046, val loss 2.5059\n",
      "step 500: train loss 2.4792, val loss 2.4757\n",
      "step 600: train loss 2.4517, val loss 2.4579\n",
      "step 700: train loss 2.4348, val loss 2.4243\n",
      "step 800: train loss 2.4175, val loss 2.4171\n",
      "step 900: train loss 2.4034, val loss 2.3982\n",
      "step 1000: train loss 2.3703, val loss 2.3810\n",
      "step 1100: train loss 2.3578, val loss 2.3700\n",
      "step 1200: train loss 2.3402, val loss 2.3479\n",
      "step 1300: train loss 2.3221, val loss 2.3261\n",
      "step 1400: train loss 2.3043, val loss 2.3062\n",
      "step 1500: train loss 2.2768, val loss 2.2959\n",
      "step 1600: train loss 2.2757, val loss 2.2809\n",
      "step 1700: train loss 2.2474, val loss 2.2574\n",
      "step 1800: train loss 2.2322, val loss 2.2483\n",
      "step 1900: train loss 2.2191, val loss 2.2252\n",
      "step 2000: train loss 2.1939, val loss 2.2150\n",
      "step 2100: train loss 2.1825, val loss 2.2089\n",
      "step 2200: train loss 2.1748, val loss 2.2035\n",
      "step 2300: train loss 2.1622, val loss 2.1843\n",
      "step 2400: train loss 2.1566, val loss 2.1825\n",
      "step 2500: train loss 2.1271, val loss 2.1625\n",
      "step 2600: train loss 2.1208, val loss 2.1535\n",
      "step 2700: train loss 2.1160, val loss 2.1420\n",
      "step 2800: train loss 2.1062, val loss 2.1293\n",
      "step 2900: train loss 2.0807, val loss 2.1245\n",
      "step 3000: train loss 2.0641, val loss 2.1033\n",
      "step 3100: train loss 2.0681, val loss 2.1094\n",
      "step 3200: train loss 2.0524, val loss 2.0878\n",
      "step 3300: train loss 2.0388, val loss 2.0937\n",
      "step 3400: train loss 2.0329, val loss 2.0759\n",
      "step 3500: train loss 2.0260, val loss 2.0794\n",
      "step 3600: train loss 2.0166, val loss 2.0614\n",
      "step 3700: train loss 2.0051, val loss 2.0629\n",
      "step 3800: train loss 1.9980, val loss 2.0474\n",
      "step 3900: train loss 1.9955, val loss 2.0447\n",
      "step 4000: train loss 1.9805, val loss 2.0402\n",
      "step 4100: train loss 1.9769, val loss 2.0341\n",
      "step 4200: train loss 1.9688, val loss 2.0349\n",
      "step 4300: train loss 1.9603, val loss 2.0234\n",
      "step 4400: train loss 1.9469, val loss 2.0237\n",
      "step 4500: train loss 1.9584, val loss 2.0332\n",
      "step 4600: train loss 1.9397, val loss 2.0218\n",
      "step 4700: train loss 1.9275, val loss 2.0199\n",
      "step 4800: train loss 1.9198, val loss 2.0082\n",
      "step 4900: train loss 1.9102, val loss 1.9994\n",
      "step 5000: train loss 1.9030, val loss 2.0006\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_iters):\n",
    "    if i == 0 or (i+1)%eval_interval == 0 or i == max_iters-1:\n",
    "        losses = estimate_loss(gpt, {'train': train_data, 'val': val_data}, eval_iters, batch_size, block_size, device)\n",
    "        print(f\"step {i+1}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size, device)\n",
    "\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt.state_dict(), \"./ckpt/gpt.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
